Refer langchain.ipynb

1. Calling OpenAI via Langchain (from langchain_openai import ChatOpenAI)

2. Prompt (from langchain.prompts import PromptTemplate)
--> to run the prompt we need CHAIN with the LLM(OpenAI LLM) (from langchain.chains import LLMChain)

3. When we use more than one prompt we use "Simple Sequential Chain" (from langchain.chains import SimpleSequentialChain) 
--> Problem is it will return only last results.

4. To solve the issue we use "Sequential  Chain" (from langchain.chains import SequentialChain)
--> This will show all the results of the chains. Lets say I have 2 chains then it will show all the Output of the chain.

5. Chatmodels with OpenAI LLM (Refer langchain.ipynb)
-- There are 3 schemas we need to looking at: 
--- HumanMessage(Human Input), 
--- SystemMessage(Instruction to the chatbot),
--- AIMessage (Output of the chatbot)

6. Prompt template + LLM + Output Parse (Refer langchain.ipynb)
---  PromptTemplate --> Here chatprompt is the prompt along with system and human message
--- LLM --> OpenAI LLM or Hugging Face Models --> chatllm is the chat llm from openai
--- Output Parse --> I want to modify any output of an llm before hand --> commaseperatedoutput is the class from the Output parser
---- We will create 3 chains (output_parser_chain = chatprompt | chat_llm | Commaseperatedoutput() )


7. Vector Search with Langchain,Cassandra,Astra DB,Vector Database
__ Need to do hands on


// If we want to keep the results in the memory, Langchain provides  "Memory Chain" for keeping the information in the memory.



---- END TO END APP and Deploying Hugging Face Space.----
1. Env set up and OpenAI Key -- Done 
2. Building simple application with 
    --> LLM's and Chatmodels --- Done
    --> Prompt Templates
    --> Output Parser (Prompt template + LLM + OutputParser)