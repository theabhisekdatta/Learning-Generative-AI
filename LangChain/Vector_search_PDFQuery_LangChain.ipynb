{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgOhk8U4Rpl"
      },
      "source": [
        "# Quickstart: Querying PDF With Astra and LangChain\n",
        "\n",
        "### A question-answering demo using Astra DB and LangChain, powered by Vector Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqfJKgRM4Rpo"
      },
      "source": [
        "#### Pre-requisites:\n",
        "\n",
        "You need a **_Serverless Cassandra with Vector Search_** database on [Astra DB](https://astra.datastax.com) to run this demo. As outlined in more detail [here](https://docs.datastax.com/en/astra-serverless/docs/vector-search/quickstart.html#_prepare_for_using_your_vector_database), you should get a DB Token with role _Database Administrator_ and copy your Database ID: these connection parameters are needed momentarily.\n",
        "\n",
        "You also need an [OpenAI API Key](https://cassio.org/start_here/#llm-access) for this demo to work.\n",
        "\n",
        "#### What you will do:\n",
        "\n",
        "- Setup: import dependencies, provide secrets, create the LangChain vector store;\n",
        "- Run a Question-Answering loop retrieving the relevant headlines and having an LLM construct the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_FeN-Ep4Rpp"
      },
      "source": [
        "Install the required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uk0qUhJUQrkO"
      },
      "outputs": [],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQN-L2J4Rpq"
      },
      "source": [
        "Import the packages you'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V4qBIihE4Rpq"
      },
      "outputs": [],
      "source": [
        "# LangChain components to use\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper  # Wraping the vectors, we can use it quickly\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings # For embedding\n",
        "\n",
        "# Support for dataset retrieval with Hugging Face\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
        "# you will also initialize the DB connection:\n",
        "import cassio # Help us to intrgrated with Cassandra Db and db connections. Here Apache Cassandra product is astra db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIs76OPQ6JyD",
        "outputId": "2464981f-aea9-499d-ccb4-5f43ea76061d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        " # For reading pdf\n",
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1itBNL1v6N9-"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu2UauiC4Rpr"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eqpM6I854Rpr"
      },
      "outputs": [],
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = \"\" # comming from Generate Token --- enter the \"AstraCS:...\" string found in in your Token JSON file\n",
        "ASTRA_DB_ID = \"\" # enter your Database ID (from the top)\n",
        "\n",
        "OPENAI_API_KEY = \"\" # enter your OpenAI key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1cmD5EF4Rpr"
      },
      "source": [
        "#### Provide your secrets:\n",
        "\n",
        "Replace the following with your Astra DB connection details and your OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "waVKJW-n6jqJ"
      },
      "outputs": [],
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('Abhisek-Datta-Resume_LL.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "42BKuFRO6meP"
      },
      "outputs": [],
      "source": [
        "# Here we will extract all the text from the PDF\n",
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text() # Extract the text\n",
        "    if content:\n",
        "        raw_text += content # Concat in a single variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "vR41Iq-4ZHnG",
        "outputId": "861bc27a-fd4d-47f9-f722-8e365a6fd030"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"PROFILE SUMMARY\\n•With 3 years of experience, I fully utilize Data Science techniques to analyze and understand data effectively and efficiently.\\n•Hands on expertise in Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, and Generative AI,\\napplying these methods to solve real-world problems.\\n•Proficient in Python for designing and developing algorithms.\\n•I'm dedicated to learning and staying updated with the latest trends, contributing to innovative projects and fostering\\nteamwork for effective problem-solving.\\nWORK EXPERIENCE\\nCapgemini India\\n•Smart Risk Monitoring: Enhancing Employee Communication Monitoring\\n•Data Integration and Embedding: Implemented a comprehensive data integration strategy following a RAG architecture. \\nInitially, sensitive documents like chats and call records undergo metadata formation before proceeding to embedding and \\nstorage in the vector space to establish a knowledge base. This approach ensures the security and integrity of sensitive \\ninformation while optimizing storage and retrieval processes.\\n•Keyword-Driven Filtering with Cosine Similarity: Utilizing Streamlit for a user-friendly interface, the system accepts \\nnew documents and keywords as queries. The documents undergo embedding, and relevant documents are retrieved based \\non the keywords provided. Cosine similarity is then computed between the new documents and the retrieved documents, \\nestablishing the risk score of the new documents. This process enhances information identification, aiding in improved \\ndecision-making and proactive risk management.\\n•Insight Generation and Flagging with GPT Model: Integrating the OpenAI/GPT-3.5 model to analyze communication \\nrecords alongside user-provided keywords facilitates the generation of insights to flag conversations for proactive risk \\nmanagement and decision-making support. When the risk score derived from cosine similarity surpasses a defined \\nthreshold, the query, new document, prompt and relevant sensitive document from the vector space undergo analysis by the \\nGPT-3.5 Language Model. The model synthesizes a detailed summary of the sensitive document without hallucination, \\nproviding valuable context for risk assessment and mitigation strategies. These initiatives yield enhanced efficiency, \\nstreamlined processes, and improved data analysis capabilities within the communication infrastructure, contributing to \\noverall organizational effectiveness and performance.\\n•Virtual Assistant: Building a virtual assistant by LSTM and Bidirectional LSTM\\n•Data Collection & Preprocessing: Gathered conversations provided by client and preprocessed the text by removing \\nunnecessary characters and symbols.Tokenism the text into words or sub words for embedding and model training.\\n•Word Embedding with Word2Vec: Utilized continuous bag of words (CBOW) technique to convert words into dense \\nvectors. Leveraging Google Pre-Trained News Data for semantic embedding and relationship.\\n•Model Architecture & Training: Implemented a sequence-to-sequence LSTM neural network architecture with encoder-\\ndecoder framework. Initialized LSTM-based model with word embedding for semantic understanding. Trained the model \\nusing conversations dataset, optimizing parameters with back propagation through time (BPTT) and gradient descent.\\n•Evaluation & Challenges: Evaluated model performance on various question types. Identified strengths in handling simple \\nqueries but struggled with complexity.\\n•Enhancement & Future Directions: Implemented bidirectional LSTM layers to improve context understanding. Suggested \\nongoing enhancements for better performance and broader capabilities.\\n•Application Tracking System: Building a ATS System\\n•Users input job description and resume in readable format without unnecessary formatting.\\n•The system leverages advanced techniques such as cosine similarities and word embedding to compare the job description \\nwith the applicant's resume.\\n•Cosine similarities serve as a crucial metric in evaluating the alignment between the job requirements and the qualifications \\nof the applicant. By measuring the cosine of the angle between two vectors representing the job description and the resume, \\nthe system can determine the degree of similarity between them.\\n•Word embedding, such as CBOW (Continuous Bag of Words), transforms words into dense vectors to capture semantic \\nrelationships. This process enables the system to convert textual data into numerical representations, facilitating precise \\ncomparisons between job descriptions and resumes.\\n•Once the comparison is performed, the system generates a comprehensive summary based on the word frequencies \\nextracted from both the job description and the resume. This summary provides users with a concise overview of the key \\nterms and phrases that are common between the two documents, facilitating a quick assessment of their relevance. I have \\ndeployed it in the Hugging Face space, ensuring wider usability and accessibility to users.\\nPersistent Systems, India\\n•Data Analyst, Dev Ops and Data Archival:\\n•Implementing Python scripting for automating ETL tasks and debugging procedures to ensure seamless data archival.\\n•Leveraging SQL for building stored procedures, conducting bench marking, and addressing bugs to maintain data integrity \\nand optimize platform performance.Abhisek Datta\\nAssociate Consultant\\nabhi.datta07@gmail.com +91 7980982884 dattaabhisek theabhisekdatta\\nMar\\xa02022 – present | Kolkata, India\\nAug\\xa02021 – Mar\\xa02022 | Pune, IndiaSKILLSET\\nMachine Learning Algorithms\\nLinear Regression, Logistic Regression, KNN, Decision Tree, Clustering, DBScan, Random Forest, Adaboost, Performance \\nMetrics, Performance Tuning\\nDeep Learning Algorithms\\nActivation Functions, Optimizer Loss Functions, Artificial Neural Network, Convolution Neural Network, Recurrent Neural \\nNetwork, Transfer Learning\\nNatural Language Processing / Large Language Model / Generative AI\\nText preprocessing, Bag of Words, Embedding, LSTM, Bidirectional LSTM, Encoder & Decoder,  Self Attention Model, \\nTransformer, RAG Architecture\\nFrameworks\\nTensorFlow, LangChain, Streamlit, Flask, FastAI\\nTools & Technologies\\nGit, VS Code, Hugging Face Space, SQL Server, OpenAI\\nCloud Technologies\\nAzure, Dataiku\\nProgramming Languages\\nPython, SQL\\nCERTIFICATES\\nAzure Fundamental\\nDataiku Core DesignerAzure AI Fundamental\\nDataiku DesignerDataIku ML Practitioner\\nDataiku Developer\\nACADEMIC DETAILS\\nMaster's in Data Science, University of Kalyani\\nAutomobile Engineering, Kiit University\\n2019 – 2021 | Nadia, West\\xa0Bengal\\n2013 – 2017 | Bhubaneswar, Orisa\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0GgIQs4Rps"
      },
      "source": [
        "Initialize the connection to your database:\n",
        "\n",
        "_(do not worry if you see a few warnings, it's just that the drivers are chatty about negotiating protocol versions with the DB.)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFBR5HnZSPmK",
        "outputId": "5b4bb3ea-6be3-4d7a-c535-88715fa67c13"
      },
      "outputs": [],
      "source": [
        "# Initializing the connection to the astra database\n",
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN, database_id=ASTRA_DB_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7NxZYb4Rps"
      },
      "source": [
        "Create the LangChain embedding and LLM objects for later usage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TavS0AK2SLrL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abhisek/Documents/Careers/Studies/Data Science/Projects/Learning-Generative-AI/LangChain/langenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/home/abhisek/Documents/Careers/Studies/Data Science/Projects/Learning-Generative-AI/LangChain/langenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY) ## This the OpenAI LLM\n",
        "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) ## This the OpenAI Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HMMx5Pm4Rpt"
      },
      "source": [
        "Create your LangChain vector store ... backed by Astra DB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bg9VAk4USQvU"
      },
      "outputs": [],
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding=embedding,  # This is the embedding. so it will convert all the text into embedding\n",
        "    table_name=\"qa_resume_demo\", # This is the Table name\n",
        "    session=None,\n",
        "    keyspace=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Still we are not converting to vector from text. We will convert when we push the data to DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9FMAhKr77AVO"
      },
      "outputs": [],
      "source": [
        "### For Text Chucks\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\", # Seperator\n",
        "    chunk_size = 200, # no of characters (token size)\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8BDHAyT7Gjr",
        "outputId": "7833f6ac-bd97-40d6-fcbe-94a81b4dd6ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PROFILE SUMMARY\\n•With 3 years of experience, I fully utilize Data Science techniques to analyze and understand data effectively and efficiently.',\n",
              " '•Hands on expertise in Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, and Generative AI,\\napplying these methods to solve real-world problems.',\n",
              " \"•Proficient in Python for designing and developing algorithms.\\n•I'm dedicated to learning and staying updated with the latest trends, contributing to innovative projects and fostering\",\n",
              " 'teamwork for effective problem-solving.\\nWORK EXPERIENCE\\nCapgemini India\\n•Smart Risk Monitoring: Enhancing Employee Communication Monitoring',\n",
              " '•Data Integration and Embedding: Implemented a comprehensive data integration strategy following a RAG architecture.',\n",
              " 'Initially, sensitive documents like chats and call records undergo metadata formation before proceeding to embedding and',\n",
              " 'storage in the vector space to establish a knowledge base. This approach ensures the security and integrity of sensitive \\ninformation while optimizing storage and retrieval processes.',\n",
              " '•Keyword-Driven Filtering with Cosine Similarity: Utilizing Streamlit for a user-friendly interface, the system accepts',\n",
              " 'new documents and keywords as queries. The documents undergo embedding, and relevant documents are retrieved based',\n",
              " 'on the keywords provided. Cosine similarity is then computed between the new documents and the retrieved documents,',\n",
              " 'establishing the risk score of the new documents. This process enhances information identification, aiding in improved \\ndecision-making and proactive risk management.',\n",
              " 'decision-making and proactive risk management.\\n•Insight Generation and Flagging with GPT Model: Integrating the OpenAI/GPT-3.5 model to analyze communication',\n",
              " 'records alongside user-provided keywords facilitates the generation of insights to flag conversations for proactive risk',\n",
              " 'management and decision-making support. When the risk score derived from cosine similarity surpasses a defined',\n",
              " 'threshold, the query, new document, prompt and relevant sensitive document from the vector space undergo analysis by the',\n",
              " 'GPT-3.5 Language Model. The model synthesizes a detailed summary of the sensitive document without hallucination,',\n",
              " 'providing valuable context for risk assessment and mitigation strategies. These initiatives yield enhanced efficiency,',\n",
              " 'streamlined processes, and improved data analysis capabilities within the communication infrastructure, contributing to \\noverall organizational effectiveness and performance.',\n",
              " '•Virtual Assistant: Building a virtual assistant by LSTM and Bidirectional LSTM\\n•Data Collection & Preprocessing: Gathered conversations provided by client and preprocessed the text by removing',\n",
              " 'unnecessary characters and symbols.Tokenism the text into words or sub words for embedding and model training.',\n",
              " '•Word Embedding with Word2Vec: Utilized continuous bag of words (CBOW) technique to convert words into dense \\nvectors. Leveraging Google Pre-Trained News Data for semantic embedding and relationship.',\n",
              " '•Model Architecture & Training: Implemented a sequence-to-sequence LSTM neural network architecture with encoder-',\n",
              " 'decoder framework. Initialized LSTM-based model with word embedding for semantic understanding. Trained the model',\n",
              " 'using conversations dataset, optimizing parameters with back propagation through time (BPTT) and gradient descent.',\n",
              " '•Evaluation & Challenges: Evaluated model performance on various question types. Identified strengths in handling simple \\nqueries but struggled with complexity.',\n",
              " 'queries but struggled with complexity.\\n•Enhancement & Future Directions: Implemented bidirectional LSTM layers to improve context understanding. Suggested',\n",
              " 'ongoing enhancements for better performance and broader capabilities.\\n•Application Tracking System: Building a ATS System',\n",
              " '•Users input job description and resume in readable format without unnecessary formatting.',\n",
              " \"•The system leverages advanced techniques such as cosine similarities and word embedding to compare the job description \\nwith the applicant's resume.\",\n",
              " \"with the applicant's resume.\\n•Cosine similarities serve as a crucial metric in evaluating the alignment between the job requirements and the qualifications\",\n",
              " 'of the applicant. By measuring the cosine of the angle between two vectors representing the job description and the resume, \\nthe system can determine the degree of similarity between them.',\n",
              " '•Word embedding, such as CBOW (Continuous Bag of Words), transforms words into dense vectors to capture semantic',\n",
              " 'relationships. This process enables the system to convert textual data into numerical representations, facilitating precise \\ncomparisons between job descriptions and resumes.',\n",
              " 'comparisons between job descriptions and resumes.\\n•Once the comparison is performed, the system generates a comprehensive summary based on the word frequencies',\n",
              " 'extracted from both the job description and the resume. This summary provides users with a concise overview of the key',\n",
              " 'terms and phrases that are common between the two documents, facilitating a quick assessment of their relevance. I have',\n",
              " 'deployed it in the Hugging Face space, ensuring wider usability and accessibility to users.\\nPersistent Systems, India\\n•Data Analyst, Dev Ops and Data Archival:',\n",
              " '•Data Analyst, Dev Ops and Data Archival:\\n•Implementing Python scripting for automating ETL tasks and debugging procedures to ensure seamless data archival.',\n",
              " '•Leveraging SQL for building stored procedures, conducting bench marking, and addressing bugs to maintain data integrity \\nand optimize platform performance.Abhisek Datta\\nAssociate Consultant',\n",
              " 'Associate Consultant\\nabhi.datta07@gmail.com +91 7980982884 dattaabhisek theabhisekdatta\\nMar\\xa02022 – present | Kolkata, India\\nAug\\xa02021 – Mar\\xa02022 | Pune, IndiaSKILLSET\\nMachine Learning Algorithms',\n",
              " 'Machine Learning Algorithms\\nLinear Regression, Logistic Regression, KNN, Decision Tree, Clustering, DBScan, Random Forest, Adaboost, Performance \\nMetrics, Performance Tuning\\nDeep Learning Algorithms',\n",
              " 'Deep Learning Algorithms\\nActivation Functions, Optimizer Loss Functions, Artificial Neural Network, Convolution Neural Network, Recurrent Neural \\nNetwork, Transfer Learning',\n",
              " 'Network, Transfer Learning\\nNatural Language Processing / Large Language Model / Generative AI',\n",
              " 'Text preprocessing, Bag of Words, Embedding, LSTM, Bidirectional LSTM, Encoder & Decoder,  Self Attention Model, \\nTransformer, RAG Architecture\\nFrameworks',\n",
              " 'Transformer, RAG Architecture\\nFrameworks\\nTensorFlow, LangChain, Streamlit, Flask, FastAI\\nTools & Technologies\\nGit, VS Code, Hugging Face Space, SQL Server, OpenAI\\nCloud Technologies\\nAzure, Dataiku',\n",
              " 'Cloud Technologies\\nAzure, Dataiku\\nProgramming Languages\\nPython, SQL\\nCERTIFICATES\\nAzure Fundamental\\nDataiku Core DesignerAzure AI Fundamental\\nDataiku DesignerDataIku ML Practitioner\\nDataiku Developer',\n",
              " \"Dataiku Developer\\nACADEMIC DETAILS\\nMaster's in Data Science, University of Kalyani\\nAutomobile Engineering, Kiit University\\n2019 – 2021 | Nadia, West\\xa0Bengal\\n2013 – 2017 | Bhubaneswar, Orisa\"]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1WK54-74Rpt"
      },
      "source": [
        "### Load the dataset into the vector store\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX5BECsdSUUM",
        "outputId": "cdff3467-8af3-45cd-f750-f3174bc521fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserted 47 headlines.\n"
          ]
        }
      ],
      "source": [
        "## Loadin to astra database\n",
        "astra_vector_store.add_texts(texts)\n",
        "\n",
        "print(\"Inserted %i headlines.\" % len(texts))\n",
        "## Indexing\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLJp8yPF4Rpt"
      },
      "source": [
        "### Run the QA cycle\n",
        "\n",
        "Simply run the cells and ask a question -- or `quit` to stop. (you can also stop execution with the \"▪\" button on the top toolbar)\n",
        "\n",
        "Here are some suggested questions:\n",
        "- _What is the experience and all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJugrh7SX3C",
        "outputId": "10e8f954-a113-47a2-a84c-615a9f6e5dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "QUESTION: \"Machine Learning\"\n",
            "ANSWER: \"Machine Learning is a subset of Artificial Intelligence that involves the use of algorithms and statistical models to enable computer systems to learn and improve from experience without explicitly being programmed. It involves training a computer model on a large dataset and using that model to make predictions or decisions on new data. Some common Machine Learning algorithms include Linear Regression, Logistic Regression, KNN, Decision Tree, Clustering, DBScan, Random Forest, Adaboost, and others. Performance metrics and performance tuning are also important aspects of Machine Learning.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.9377] \"Network, Transfer Learning\n",
            "Natural Language Processing / Large Language Model / Generative AI ...\"\n",
            "    [0.9374] \"Machine Learning Algorithms\n",
            "Linear Regression, Logistic Regression, KNN, Decision Tree, Clustering, DBScan, Random Forest, Adaboost, Performance \n",
            "Metrics, Performance Tuning\n",
            "Deep Learning Algorithms ...\"\n",
            "    [0.9278] \"•Hands on expertise in Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, and Generative AI,\n",
            "applying these methods to solve real-world problems. ...\"\n",
            "    [0.9224] \"Deep Learning Algorithms\n",
            "Activation Functions, Optimizer Loss Functions, Artificial Neural Network, Convolution Neural Network, Recurrent Neural \n",
            "Network, Transfer Learning ...\"\n",
            "\n",
            "QUESTION: \"Deep Learning\"\n",
            "ANSWER: \"Deep Learning is a subset of machine learning that uses artificial neural networks to learn from large amounts of data and make predictions or decisions. It is often used for tasks such as image and speech recognition, natural language processing, and generative modeling. Some key components of deep learning include activation functions, optimizer loss functions, and transfer learning.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.9371] \"Network, Transfer Learning\n",
            "Natural Language Processing / Large Language Model / Generative AI ...\"\n",
            "    [0.9368] \"Deep Learning Algorithms\n",
            "Activation Functions, Optimizer Loss Functions, Artificial Neural Network, Convolution Neural Network, Recurrent Neural \n",
            "Network, Transfer Learning ...\"\n",
            "    [0.9274] \"•Hands on expertise in Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, and Generative AI,\n",
            "applying these methods to solve real-world problems. ...\"\n",
            "    [0.9142] \"Machine Learning Algorithms\n",
            "Linear Regression, Logistic Regression, KNN, Decision Tree, Clustering, DBScan, Random Forest, Adaboost, Performance \n",
            "Metrics, Performance Tuning\n",
            "Deep Learning Algorithms ...\"\n",
            "\n",
            "QUESTION: \"Is the candidate sutable for Data Sciece role?\"\n",
            "ANSWER: \"Yes, the candidate seems suitable for a Data Science role based on their 3 years of experience in utilizing Data Science techniques and their Master's degree in Data Science. Additionally, their experience as a Data Analyst and their skills in Machine Learning Algorithms make them a strong candidate for a Data Science role.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.9129] \"PROFILE SUMMARY\n",
            "•With 3 years of experience, I fully utilize Data Science techniques to analyze and understand data effectively and efficiently. ...\"\n",
            "    [0.9096] \"Dataiku Developer\n",
            "ACADEMIC DETAILS\n",
            "Master's in Data Science, University of Kalyani\n",
            "Automobile Engineering, Kiit University\n",
            "2019 – 2021 | Nadia, West Bengal\n",
            "2013 – 2017 | Bhubaneswar, Orisa ...\"\n",
            "    [0.9065] \"Associate Consultant\n",
            "abhi.datta07@gmail.com +91 7980982884 dattaabhisek theabhisekdatta\n",
            "Mar 2022 – present | Kolkata, India\n",
            "Aug 2021 – Mar 2022 | Pune, IndiaSKILLSET\n",
            "Machine Learning Algorithms ...\"\n",
            "    [0.9044] \"•Data Analyst, Dev Ops and Data Archival:\n",
            "•Implementing Python scripting for automating ETL tasks and debugging procedures to ensure seamless data archival. ...\"\n"
          ]
        }
      ],
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "     ## Questions comes to query_text, along with LLM. astra_vector_index.query help to do that\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip() \n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSaUPguw389l"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
