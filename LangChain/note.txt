Refer langchain.ipynb

1. Calling OpenAI via Langchain (from langchain_openai import ChatOpenAI)

2. Prompt (from langchain.prompts import PromptTemplate)
--> to run the prompt we need CHAIN with the LLM(OpenAI LLM) (from langchain.chains import LLMChain)

3. When we use more than one prompt we use "Simple Sequential Chain" (from langchain.chains import SimpleSequentialChain) 
--> Problem is it will return only last results.

4. To solve the issue we use "Sequential  Chain" (from langchain.chains import SequentialChain)
--> This will show all the results of the chains. Lets say I have 2 chains then it will show all the Output of the chain.

5. Chatmodels with OpenAI LLM (Refer langchain.ipynb)
-- There are 3 schemas we need to looking at: 
--- HumanMessage(Human Input), 
--- SystemMessage(Instruction to the chatbot),
--- AIMessage (Output of the chatbot)

6. Prompt template + LLM + Output Parse (Refer langchain.ipynb)
---  PromptTemplate --> Here chatprompt is the prompt along with system and human message
--- LLM --> OpenAI LLM or Hugging Face Models --> chatllm is the chat llm from openai
--- Output Parse --> I want to modify any output of an llm before hand --> commaseperatedoutput is the class from the Output parser
---- We will create 3 chains (output_parser_chain = chatprompt | chat_llm | Commaseperatedoutput() )


7. Vector Search with Langchain,Cassandra,Astra DB,Vector Database (Refer Vector_search_PDFQuery_LangChain.ipynb)
__ Need to do hands on
    What is Vector Search???
--- When ever we need to Q and A from the huge PDF we need vector search
--- First reading a huge pdf let say and convert to text, we will then convert into various text chunks.
--- We will convert the chucks into Text Embeddings with the help of OpenAI Embeddings (Text to Vectors)
--- We will store all the Text Embeddings into some kind of database (Cassandra DB) Create a Database.
--- Cassandra DB is a nosql database made by apache, it has scalability and high availablity.
--- From the Cassandra DB we can perform vector search.
--- So a human whenever text query then it will perform a similarity and Embeddings and will get the response



// If we want to keep the results in the memory, Langchain provides  "Memory Chain" for keeping the information in the memory.



---- END TO END APP and Deploying Hugging Face Space.----
1. Env set up and OpenAI Key -- Done 
2. Building simple application with 
    --> LLM's and Chatmodels --- Done
    --> Prompt Templates
    --> Output Parser (Prompt template + LLM + OutputParser)